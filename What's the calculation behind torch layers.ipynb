{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe3f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full output\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = 'all' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c2f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c51fb",
   "metadata": {},
   "source": [
    "# Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b7d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(10,4) # size (10,4) in neural networks mean there're 10 learning examples in the inputs, each with 4 features\n",
    "fc = nn.Linear(in_features= 4, out_features = 1) # in_features should be equal to inputs.shape[1], the out_features is the number of features of the target\n",
    "outputs = fc(inputs) # hence, the outputs would in the size of (10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8681ddc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3102,  0.1271, -0.0823,  1.0858],\n",
       "        [-1.1686,  1.6650,  1.3797, -0.9915],\n",
       "        [ 0.3897,  0.3723,  1.0115,  0.1784],\n",
       "        [ 1.9373, -1.0086,  0.7447,  0.2432],\n",
       "        [ 0.4305,  0.0627,  0.7668, -1.9446],\n",
       "        [ 1.0522, -0.2573,  0.6940,  0.0913],\n",
       "        [-0.0278, -0.0746, -0.0560, -1.1732],\n",
       "        [-1.1034,  0.8108, -1.1170, -0.2516],\n",
       "        [-1.1515,  1.2772, -1.6101, -0.8164],\n",
       "        [-0.6352,  0.9345,  0.4916,  1.1171]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2397],\n",
       "        [-0.9325],\n",
       "        [-0.7593],\n",
       "        [-0.8897],\n",
       "        [-1.2447],\n",
       "        [-0.7963],\n",
       "        [-0.7541],\n",
       "        [-0.1225],\n",
       "        [-0.1838],\n",
       "        [-0.2319]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs\n",
    "outputs\n",
    "\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e9aba6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2009, -0.0747, -0.2266,  0.2618]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4708], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the weights and biases in this 'fc' linear layer\n",
    "fc.weight\n",
    "fc.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d23b0f",
   "metadata": {},
   "source": [
    "The algorithm behind the linear layer is actually some basic matrix calculus:\n",
    "$$outputs = inputs \\times weight^T + bias$$\n",
    "Let's manually implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2c1a59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2397],\n",
       "        [-0.9325],\n",
       "        [-0.7593],\n",
       "        [-0.8897],\n",
       "        [-1.2447],\n",
       "        [-0.7963],\n",
       "        [-0.7541],\n",
       "        [-0.1225],\n",
       "        [-0.1838],\n",
       "        [-0.2319]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(inputs,fc.weight.T).add(fc.bias) # Here we get the same outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7115f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4864],\n",
       "        [ 1.7685],\n",
       "        [ 0.5358],\n",
       "        [-0.1849],\n",
       "        [ 1.2545],\n",
       "        [ 0.1672],\n",
       "        [ 0.4731],\n",
       "        [-0.0845],\n",
       "        [ 0.1134],\n",
       "        [ 0.1043]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4864],\n",
       "        [ 1.7685],\n",
       "        [ 0.5358],\n",
       "        [-0.1849],\n",
       "        [ 1.2545],\n",
       "        [ 0.1672],\n",
       "        [ 0.4731],\n",
       "        [-0.0845],\n",
       "        [ 0.1134],\n",
       "        [ 0.1043]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1 = nn.Linear(in_features=4, out_features=1, bias=False) # We could remove the bias by setting it to False\n",
    "fc1(inputs)\n",
    "torch.mm(inputs,fc1.weight.T) # Again we get the same outputs as fc1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53969e12",
   "metadata": {},
   "source": [
    "# Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d67565",
   "metadata": {},
   "source": [
    "The difference among Conv1D, Conv2D, Conv3D is the input of these layers\n",
    "* Conv1D is used for input signals which are similar to the voice, with the input shape (batch_size, W, channels)\n",
    "* Conv2D is used for images, with the input shape (batch_size, H, W, channels)\n",
    "* Conv3D is usually used for videos where you have a frame for each time span, with the input shape (batch_size, H, W, d, channels)\n",
    "\n",
    "Here we focus on Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ffef57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 16, 59, 79])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=2)\n",
    "\n",
    "inputs = torch.rand(100,3,120,160) # the inputs should be in the shape of [batch_size,in_channels,height,width]\n",
    "outputs = conv2d(inputs)\n",
    "\n",
    "outputs.shape # [batch_size, out_channels, height, width]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6140b",
   "metadata": {},
   "source": [
    "You may be wondering how could we calculate the outputs height and width, well, here is the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "777b969e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: [100, 16, 59, 79]\n",
      "Weight/kernel size: [16, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# inputs shape: [batch_size, in_channels, input_height, input_width]\n",
    "# conv2d parameters: in_channels, out_channels, kernel_size, stride, dilation\n",
    "# outputs shape: [batch_size, out_channels, output_height, output_width]\n",
    "\n",
    "batch_size, in_channels, input_height, input_width = 100, 3, 120, 160\n",
    "out_channels, kernel_size, stride, padding, dilation = 16, [3,3], [2,2], [0,0], [1,1]\n",
    "# the default value for stride, padding, dilation are [1,1], [0,0], [1,1]\n",
    "\n",
    "\n",
    "output_height = int(np.floor((input_height + 2*padding[0] - dilation[0]*(kernel_size[0]-1)-1)/stride[0] + 1))\n",
    "output_width = int(np.floor((input_width + 2*padding[1] - dilation[1]*(kernel_size[1]-1)-1)/stride[1] + 1))\n",
    "\n",
    "print(\"Output size: {}\".format([batch_size, out_channels, output_height, output_width]))\n",
    "print(\"Weight/kernel size: {}\".format([out_channels, in_channels, kernel_size[0], kernel_size[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc6413b",
   "metadata": {},
   "source": [
    "# Max Pool layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99192e7b",
   "metadata": {},
   "source": [
    "nn.MaxPool2d applies a 2D max pooling over an input signal composed of several input planes. inputs shape is [batch_size, in_channels, image_height, image_width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882808ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 40, 80])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxpool = nn.MaxPool2d(kernel_size=(3,2), padding=0, dilation=1)\n",
    "inputs = torch.rand(100,3,120,160) # shape: [batch_size, in_channels, height, width]\n",
    "outputs = maxpool(inputs)\n",
    "\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d9a60",
   "metadata": {},
   "source": [
    "Here is the formula to calculate the outputs height and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40d184b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: [100, 3, 40, 80]\n"
     ]
    }
   ],
   "source": [
    "## ------------ MaxPoll2d --------------- ## \n",
    "\n",
    "# inputs shape: [batch_size, in_channels, input_height, input_width]\n",
    "# outputs shape: [batch_size, out_channels, output_height, output_width]\n",
    "\n",
    "batch_size, in_channels, input_height, input_width = 100,3,120,160\n",
    "kernel_size, padding, dilation = [3,2], [0,0], [1,1]\n",
    "stride = kernel_size\n",
    "# default values are: stride=kernel_size, padding=[0,0], dilation=[1,1]\n",
    "\n",
    "\n",
    "output_height = int(np.floor((input_height + 2*padding[0] - dilation[0]*(kernel_size[0]-1)-1)/stride[0] + 1))\n",
    "output_width = int(np.floor((input_width + 2*padding[1] - dilation[1]*(kernel_size[1]-1)-1)/stride[1] + 1))\n",
    "\n",
    "print(\"Output size: {}\".format([batch_size, in_channels, output_height, output_width]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12588bae",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fe347",
   "metadata": {},
   "source": [
    "## BatchNorm1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2828c36",
   "metadata": {},
   "source": [
    "BatchNorm1d usually applies Barch Normlization over a 2D input, which in the shape of [batch_size,num_features]. The mean and standard-deviation are calculated per-dimension over the mini-batches and the weight and bias are learnable parameter vetors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d757d206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2533,  0.7277,  0.9117, -0.5479, -0.6629],\n",
       "        [ 1.8718,  0.9728, -0.3642,  1.8587,  0.0546],\n",
       "        [ 0.1330,  2.1461, -1.2436, -0.4471, -0.3075],\n",
       "        [-0.5346, -0.4766, -0.5465,  0.7113, -0.3958],\n",
       "        [ 0.0586,  0.0893, -0.3842, -1.0007, -0.0269],\n",
       "        [ 0.7026, -1.0440, -1.3213,  0.3033,  0.1506],\n",
       "        [ 0.1498, -0.0740, -0.3236,  0.2132, -1.6511],\n",
       "        [-0.6272, -0.4069,  1.7836, -0.5955,  2.1632],\n",
       "        [ 0.6795, -0.4418,  1.4164,  1.1529, -0.5701],\n",
       "        [-2.1803, -1.4923,  0.0716, -1.6482,  1.2458]],\n",
       "       grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchNorm1 = nn.BatchNorm1d(5, affine=True) # the first argument 5 is the number of input features\n",
    "\n",
    "inputs = torch.randn(10, 5) # inputs is in the shape of [batch_size, num_features]\n",
    "outputs = batchNorm1(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f9a2f9",
   "metadata": {},
   "source": [
    "Let's reproduct the above calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efd73b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.8866e-01, -5.7365e-01, -9.9263e-02, -3.3903e-04, -3.8356e-01])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.2932, 0.7139, 1.5251, 1.4939, 0.5185])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.mean(dim=0) # we get the mean and standard-deviation for each column\n",
    "inputs.var(dim=0, unbiased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65736832",
   "metadata": {},
   "source": [
    "$$ y = \\frac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}}*weight + bias$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7982d9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2533,  0.7277,  0.9117, -0.5479, -0.6629],\n",
       "        [ 1.8718,  0.9728, -0.3642,  1.8587,  0.0546],\n",
       "        [ 0.1330,  2.1461, -1.2436, -0.4471, -0.3075],\n",
       "        [-0.5346, -0.4766, -0.5465,  0.7113, -0.3958],\n",
       "        [ 0.0586,  0.0893, -0.3842, -1.0007, -0.0269],\n",
       "        [ 0.7026, -1.0440, -1.3213,  0.3033,  0.1506],\n",
       "        [ 0.1498, -0.0740, -0.3236,  0.2132, -1.6511],\n",
       "        [-0.6272, -0.4069,  1.7836, -0.5955,  2.1632],\n",
       "        [ 0.6795, -0.4418,  1.4164,  1.1529, -0.5701],\n",
       "        [-2.1803, -1.4923,  0.0716, -1.6482,  1.2458]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs-inputs.mean(dim=0))/torch.sqrt(inputs.var(dim=0,unbiased=False)+batchNorm1.eps)*batchNorm1.weight + batchNorm1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f4f68",
   "metadata": {},
   "source": [
    "## BatchNorm2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e6d07",
   "metadata": {},
   "source": [
    "BatchNorm2d applies batch Normalization over a 4D input with the shape of [Batch_size, in_channels,height,width]. The mean and standard-deviation are calculated per-dimension over the channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77b27291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0825, -0.2377],\n",
       "          [ 2.4320, -0.7050]],\n",
       "\n",
       "         [[-0.8265, -0.7808],\n",
       "          [ 1.2174, -1.3867]],\n",
       "\n",
       "         [[ 0.9693, -0.7551],\n",
       "          [-0.3473, -0.7084]]],\n",
       "\n",
       "\n",
       "        [[[-0.3053, -0.4256],\n",
       "          [-1.0903,  0.2493]],\n",
       "\n",
       "         [[-0.1257,  0.1201],\n",
       "          [ 1.8087, -0.0265]],\n",
       "\n",
       "         [[-1.2152, -0.6561],\n",
       "          [ 0.9815,  1.7313]]]], grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchNorm2d = nn.BatchNorm2d(3, affine=True) # the first parameter 3 is the in_channels of the input\n",
    "\n",
    "inputs = torch.randn(2,3,2,2) # in the shape of [batch_size, in_channles, height, width]\n",
    "outputs = batchNorm2d(inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c2b98",
   "metadata": {},
   "source": [
    "Let's reproduct that with this fomular below\n",
    "$$ y = \\frac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}}*weight + bias$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "226197a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the means and vars in each channels\n",
    "\n",
    "means = torch.tensor([inputs[:,0].mean(),inputs[:,1].mean(),inputs[:,2].mean()\n",
    "                     ]).view(-1,1,1).expand_as(inputs)\n",
    "vars = torch.tensor([inputs[:,0].var(unbiased=False),inputs[:,1].var(unbiased=False),inputs[:,2].var(unbiased=False)\n",
    "                     ]).view(-1,1,1).expand_as(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe7760f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0825, -0.2377],\n",
       "          [ 2.4320, -0.7050]],\n",
       "\n",
       "         [[-0.8265, -0.7808],\n",
       "          [ 1.2174, -1.3867]],\n",
       "\n",
       "         [[ 0.9693, -0.7551],\n",
       "          [-0.3473, -0.7084]]],\n",
       "\n",
       "\n",
       "        [[[-0.3053, -0.4256],\n",
       "          [-1.0903,  0.2493]],\n",
       "\n",
       "         [[-0.1257,  0.1201],\n",
       "          [ 1.8087, -0.0265]],\n",
       "\n",
       "         [[-1.2152, -0.6561],\n",
       "          [ 0.9815,  1.7313]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchNorm2d.weight.view(-1,1,1).expand_as(inputs)*(inputs-means)/torch.sqrt(vars+batchNorm2d.eps)+batchNorm2d.bias.view(-1,1,1).expand_as(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90cde0",
   "metadata": {},
   "source": [
    "# Layer Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9855c97",
   "metadata": {},
   "source": [
    "Layer normalization is a method developed by Geoffery Hinton. Assume a mini-batch consists of multiple examples with the same number of features. Mini-batches are matrices - or tensors if each input is multi-dimensional - where one axis corresponds to the batch and the other axis - or axes - correspond to the feature dimensions. Well, layer normalization is to normalize the inputs across the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf13d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(3,3,2,2) # shape: [batch_size, in_channles, height, width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6f2e04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.9999,  0.9999],\n",
       "          [-1.0000,  1.0000]],\n",
       "\n",
       "         [[-0.9996,  0.9996],\n",
       "          [-0.9999,  0.9999]],\n",
       "\n",
       "         [[-0.9999,  0.9999],\n",
       "          [ 0.9999, -0.9999]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9987, -0.9987],\n",
       "          [-1.0000,  1.0000]],\n",
       "\n",
       "         [[ 0.9999, -0.9999],\n",
       "          [-0.9996,  0.9996]],\n",
       "\n",
       "         [[-1.0000,  1.0000],\n",
       "          [ 0.9991, -0.9991]]],\n",
       "\n",
       "\n",
       "        [[[-0.9984,  0.9984],\n",
       "          [-0.9996,  0.9996]],\n",
       "\n",
       "         [[-0.9998,  0.9998],\n",
       "          [-0.9995,  0.9995]],\n",
       "\n",
       "         [[ 0.9998, -0.9998],\n",
       "          [ 0.9995, -0.9995]]]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm1 = nn.LayerNorm([2]) # Do the LayerNorm in the last dimention of the inputs, which is width\n",
    "outputs1 = lm1(inputs)\n",
    "outputs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94bbe155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.9999,  0.9999],\n",
       "          [-1.0000,  1.0000]],\n",
       "\n",
       "         [[-0.9996,  0.9996],\n",
       "          [-0.9999,  0.9999]],\n",
       "\n",
       "         [[-0.9999,  0.9999],\n",
       "          [ 0.9999, -0.9999]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9987, -0.9987],\n",
       "          [-1.0000,  1.0000]],\n",
       "\n",
       "         [[ 0.9999, -0.9999],\n",
       "          [-0.9996,  0.9996]],\n",
       "\n",
       "         [[-1.0000,  1.0000],\n",
       "          [ 0.9991, -0.9991]]],\n",
       "\n",
       "\n",
       "        [[[-0.9984,  0.9984],\n",
       "          [-0.9996,  0.9996]],\n",
       "\n",
       "         [[-0.9998,  0.9998],\n",
       "          [-0.9995,  0.9995]],\n",
       "\n",
       "         [[ 0.9998, -0.9998],\n",
       "          [ 0.9995, -0.9995]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproduction\n",
    "lm1.weight*(inputs-inputs.mean(axis=3,keepdims=True))/torch.sqrt(inputs.var(axis=3,keepdims=True,unbiased=False)\n",
    "                                                                 +lm1.eps)+lm1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67fbda11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1601,  0.2329],\n",
       "          [-0.5780,  1.5052]],\n",
       "\n",
       "         [[ 0.1325,  0.9470],\n",
       "          [-1.6583,  0.5788]],\n",
       "\n",
       "         [[-0.8742,  0.9156],\n",
       "          [ 1.0738, -1.1152]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2108, -0.1954],\n",
       "          [-1.4071,  1.3917]],\n",
       "\n",
       "         [[ 0.1899, -1.5970],\n",
       "          [ 0.2437,  1.1634]],\n",
       "\n",
       "         [[-0.9724,  1.6525],\n",
       "          [-0.1252, -0.5548]]],\n",
       "\n",
       "\n",
       "        [[[-0.6089,  0.6907],\n",
       "          [-1.2945,  1.2127]],\n",
       "\n",
       "         [[-0.3071,  1.5888],\n",
       "          [-1.1695, -0.1123]],\n",
       "\n",
       "         [[ 0.9410, -1.3239],\n",
       "          [ 0.9933, -0.6105]]]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2 = nn.LayerNorm([2,2]) # Do the LayerNorm in the last two dimentions of the inputs, which is (height, width)\n",
    "outputs2 = lm2(inputs)\n",
    "outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85fc3552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1601,  0.2329],\n",
       "          [-0.5780,  1.5052]],\n",
       "\n",
       "         [[ 0.1325,  0.9470],\n",
       "          [-1.6583,  0.5788]],\n",
       "\n",
       "         [[-0.8742,  0.9156],\n",
       "          [ 1.0738, -1.1152]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2108, -0.1954],\n",
       "          [-1.4071,  1.3917]],\n",
       "\n",
       "         [[ 0.1899, -1.5970],\n",
       "          [ 0.2437,  1.1634]],\n",
       "\n",
       "         [[-0.9724,  1.6525],\n",
       "          [-0.1252, -0.5548]]],\n",
       "\n",
       "\n",
       "        [[[-0.6089,  0.6907],\n",
       "          [-1.2945,  1.2127]],\n",
       "\n",
       "         [[-0.3071,  1.5888],\n",
       "          [-1.1695, -0.1123]],\n",
       "\n",
       "         [[ 0.9410, -1.3239],\n",
       "          [ 0.9933, -0.6105]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproduction\n",
    "lm2.weight*(inputs-inputs.mean(axis=(2,3),keepdims=True))/torch.sqrt(inputs.var(axis=(2,3),keepdims=True,unbiased=False)\n",
    "                                                                 +lm2.eps)+lm2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89f970f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.3797,  0.0954],\n",
       "          [-0.7633,  1.4426]],\n",
       "\n",
       "         [[ 0.4641,  1.2054],\n",
       "          [-1.1656,  0.8703]],\n",
       "\n",
       "         [[-1.0089,  0.6630],\n",
       "          [ 0.8108, -1.2340]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0057, -0.3791],\n",
       "          [-1.5270,  1.1243]],\n",
       "\n",
       "         [[ 0.6382, -0.7542],\n",
       "          [ 0.6802,  1.3967]],\n",
       "\n",
       "         [[-1.3297,  1.4602],\n",
       "          [-0.4293, -0.8859]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5174,  1.1289],\n",
       "          [ 0.1948,  1.3745]],\n",
       "\n",
       "         [[-1.0579,  0.9154],\n",
       "          [-1.9555, -0.8552]],\n",
       "\n",
       "         [[ 0.5996, -1.0015],\n",
       "          [ 0.6366, -0.4972]]]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3 = nn.LayerNorm([3,2,2]) # Do the LayerNorm in the last three dimentions of the inputs, which is (in_channels, height, width)\n",
    "outputs3 = lm3(inputs)\n",
    "outputs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26cddfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.3797,  0.0954],\n",
       "          [-0.7633,  1.4426]],\n",
       "\n",
       "         [[ 0.4641,  1.2054],\n",
       "          [-1.1656,  0.8703]],\n",
       "\n",
       "         [[-1.0089,  0.6630],\n",
       "          [ 0.8108, -1.2340]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0057, -0.3791],\n",
       "          [-1.5270,  1.1243]],\n",
       "\n",
       "         [[ 0.6382, -0.7542],\n",
       "          [ 0.6802,  1.3967]],\n",
       "\n",
       "         [[-1.3297,  1.4602],\n",
       "          [-0.4293, -0.8859]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5174,  1.1289],\n",
       "          [ 0.1948,  1.3745]],\n",
       "\n",
       "         [[-1.0579,  0.9154],\n",
       "          [-1.9555, -0.8552]],\n",
       "\n",
       "         [[ 0.5996, -1.0015],\n",
       "          [ 0.6366, -0.4972]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproduction\n",
    "lm3.weight*(inputs-inputs.mean(axis=(1,2,3),keepdims=True))/torch.sqrt(inputs.var(axis=(1,2,3),keepdims=True,unbiased=False)\n",
    "                                                                 +lm3.eps)+lm3.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299aaf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
