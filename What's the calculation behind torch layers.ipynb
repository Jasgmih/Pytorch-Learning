{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe3f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full output\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = 'all' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c2f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c51fb",
   "metadata": {},
   "source": [
    "# Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b7d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(10,4) # size (10,4) in neural networks mean there're 10 learning examples in the inputs, each with 4 features\n",
    "fc = nn.Linear(in_features= 4, out_features = 1) # in_features should be equal to inputs.shape[1], the out_features is the number of features of the target\n",
    "outputs = fc(inputs) # hence, the outputs would in the size of (10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8681ddc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3102,  0.1271, -0.0823,  1.0858],\n",
       "        [-1.1686,  1.6650,  1.3797, -0.9915],\n",
       "        [ 0.3897,  0.3723,  1.0115,  0.1784],\n",
       "        [ 1.9373, -1.0086,  0.7447,  0.2432],\n",
       "        [ 0.4305,  0.0627,  0.7668, -1.9446],\n",
       "        [ 1.0522, -0.2573,  0.6940,  0.0913],\n",
       "        [-0.0278, -0.0746, -0.0560, -1.1732],\n",
       "        [-1.1034,  0.8108, -1.1170, -0.2516],\n",
       "        [-1.1515,  1.2772, -1.6101, -0.8164],\n",
       "        [-0.6352,  0.9345,  0.4916,  1.1171]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2397],\n",
       "        [-0.9325],\n",
       "        [-0.7593],\n",
       "        [-0.8897],\n",
       "        [-1.2447],\n",
       "        [-0.7963],\n",
       "        [-0.7541],\n",
       "        [-0.1225],\n",
       "        [-0.1838],\n",
       "        [-0.2319]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs\n",
    "outputs\n",
    "\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e9aba6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2009, -0.0747, -0.2266,  0.2618]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4708], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the weights and biases in this 'fc' linear layer\n",
    "fc.weight\n",
    "fc.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d23b0f",
   "metadata": {},
   "source": [
    "The algorithm behind the linear layer is actually some basic matrix calculus:\n",
    "$$outputs = inputs \\times weight^T + bias$$\n",
    "Let's manually implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2c1a59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2397],\n",
       "        [-0.9325],\n",
       "        [-0.7593],\n",
       "        [-0.8897],\n",
       "        [-1.2447],\n",
       "        [-0.7963],\n",
       "        [-0.7541],\n",
       "        [-0.1225],\n",
       "        [-0.1838],\n",
       "        [-0.2319]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(inputs,fc.weight.T).add(fc.bias) # Here we get the same outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7115f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4864],\n",
       "        [ 1.7685],\n",
       "        [ 0.5358],\n",
       "        [-0.1849],\n",
       "        [ 1.2545],\n",
       "        [ 0.1672],\n",
       "        [ 0.4731],\n",
       "        [-0.0845],\n",
       "        [ 0.1134],\n",
       "        [ 0.1043]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4864],\n",
       "        [ 1.7685],\n",
       "        [ 0.5358],\n",
       "        [-0.1849],\n",
       "        [ 1.2545],\n",
       "        [ 0.1672],\n",
       "        [ 0.4731],\n",
       "        [-0.0845],\n",
       "        [ 0.1134],\n",
       "        [ 0.1043]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1 = nn.Linear(in_features=4, out_features=1, bias=False) # We could remove the bias by setting it to False\n",
    "fc1(inputs)\n",
    "torch.mm(inputs,fc1.weight.T) # Again we get the same outputs as fc1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9465f6",
   "metadata": {},
   "source": [
    "# Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c03e0",
   "metadata": {},
   "source": [
    "The difference among Conv1D, Conv2D, Conv3D is the input of these layers\n",
    "* Conv1D is used for input signals which are similar to the voice, with the input shape (batch_size, W, channels)\n",
    "* Conv2D is used for images, with the input shape (batch_size, H, W, channels)\n",
    "* Conv3D is usually used for videos where you have a frame for each time span, with the input shape (batch_size, H, W, d, channels)\n",
    "\n",
    "Here we focus on Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ffef57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 16, 59, 79])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=2)\n",
    "\n",
    "inputs = torch.rand(100,3,120,160) # the inputs should be in the shape of [batch_size,in_channels,height,width]\n",
    "outputs = conv2d(inputs)\n",
    "\n",
    "outputs.shape # [batch_size, out_channels, height, width]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69fc09",
   "metadata": {},
   "source": [
    "You may be wondering how could we calculate the outputs height and width, well, here is the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a8ede9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: [100, 16, 59, 79]\n",
      "Weight/kernel size: [16, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# inputs shape: [batch_size, in_channels, input_height, input_width]\n",
    "# conv2d parameters: in_channels, out_channels, kernel_size, stride, dilation\n",
    "# outputs shape: [batch_size, out_channels, output_height, output_width]\n",
    "\n",
    "batch_size, in_channels, input_height, input_width = 100, 3, 120, 160\n",
    "out_channels, kernel_size, stride, padding, dilation = 16, [3,3], [2,2], [0,0], [1,1]\n",
    "# the default value for stride, padding, dilation are [1,1], [0,0], [1,1]\n",
    "\n",
    "\n",
    "output_height = int(np.floor((input_height + 2*padding[0] - dilation[0]*(kernel_size[0]-1)-1)/stride[0] + 1))\n",
    "output_width = int(np.floor((input_width + 2*padding[1] - dilation[1]*(kernel_size[1]-1)-1)/stride[1] + 1))\n",
    "\n",
    "print(\"Output size: {}\".format([batch_size, out_channels, output_height, output_width]))\n",
    "print(\"Weight/kernel size: {}\".format([out_channels, in_channels, kernel_size[0], kernel_size[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583dcbd3",
   "metadata": {},
   "source": [
    "# Max Pool layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a29916",
   "metadata": {},
   "source": [
    "nn.MaxPool2d applies a 2D max pooling over an input signal composed of several input planes. inputs shape is [batch_size, in_channels, image_height, image_width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d10ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 40, 80])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxpool = nn.MaxPool2d(kernel_size=(3,2), padding=0, dilation=1)\n",
    "inputs = torch.rand(100,3,120,160) # shape: [batch_size, in_channels, height, width]\n",
    "outputs = maxpool(inputs)\n",
    "\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b2b41",
   "metadata": {},
   "source": [
    "Here is the formula to calculate the outputs height and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "617c9267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: [100, 3, 40, 80]\n"
     ]
    }
   ],
   "source": [
    "## ------------ MaxPoll2d --------------- ## \n",
    "\n",
    "# inputs shape: [batch_size, in_channels, input_height, input_width]\n",
    "# outputs shape: [batch_size, out_channels, output_height, output_width]\n",
    "\n",
    "batch_size, in_channels, input_height, input_width = 100,3,120,160\n",
    "kernel_size, padding, dilation = [3,2], [0,0], [1,1]\n",
    "stride = kernel_size\n",
    "# default values are: stride=kernel_size, padding=[0,0], dilation=[1,1]\n",
    "\n",
    "\n",
    "output_height = int(np.floor((input_height + 2*padding[0] - dilation[0]*(kernel_size[0]-1)-1)/stride[0] + 1))\n",
    "output_width = int(np.floor((input_width + 2*padding[1] - dilation[1]*(kernel_size[1]-1)-1)/stride[1] + 1))\n",
    "\n",
    "print(\"Output size: {}\".format([batch_size, in_channels, output_height, output_width]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24696d2f",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe248c3",
   "metadata": {},
   "source": [
    "## BatchNorm1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b246212",
   "metadata": {},
   "source": [
    "BatchNorm1d usually applies Barch Normlization over a 2D input, which in the shape of [batch_size,num_features]. The mean and standard-deviation are calculated per-dimension over the mini-batches and the weight and bias are learnable parameter vetors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b529af93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2533,  0.7277,  0.9117, -0.5479, -0.6629],\n",
       "        [ 1.8718,  0.9728, -0.3642,  1.8587,  0.0546],\n",
       "        [ 0.1330,  2.1461, -1.2436, -0.4471, -0.3075],\n",
       "        [-0.5346, -0.4766, -0.5465,  0.7113, -0.3958],\n",
       "        [ 0.0586,  0.0893, -0.3842, -1.0007, -0.0269],\n",
       "        [ 0.7026, -1.0440, -1.3213,  0.3033,  0.1506],\n",
       "        [ 0.1498, -0.0740, -0.3236,  0.2132, -1.6511],\n",
       "        [-0.6272, -0.4069,  1.7836, -0.5955,  2.1632],\n",
       "        [ 0.6795, -0.4418,  1.4164,  1.1529, -0.5701],\n",
       "        [-2.1803, -1.4923,  0.0716, -1.6482,  1.2458]],\n",
       "       grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchNorm1 = nn.BatchNorm1d(5, affine=True) # the first argument 5 is the number of input features\n",
    "\n",
    "inputs = torch.randn(10, 5) # inputs is in the shape of [batch_size, num_features]\n",
    "outputs = batchNorm1(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3eacb6",
   "metadata": {},
   "source": [
    "Let's reproduct the above calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33a4d718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.8866e-01, -5.7365e-01, -9.9263e-02, -3.3903e-04, -3.8356e-01])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.2932, 0.7139, 1.5251, 1.4939, 0.5185])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.mean(dim=0) # we get the mean and standard-deviation for each column\n",
    "inputs.var(dim=0, unbiased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b79df9",
   "metadata": {},
   "source": [
    "$$ y = \\frac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}}*weight + bias$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a99e783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2533,  0.7277,  0.9117, -0.5479, -0.6629],\n",
       "        [ 1.8718,  0.9728, -0.3642,  1.8587,  0.0546],\n",
       "        [ 0.1330,  2.1461, -1.2436, -0.4471, -0.3075],\n",
       "        [-0.5346, -0.4766, -0.5465,  0.7113, -0.3958],\n",
       "        [ 0.0586,  0.0893, -0.3842, -1.0007, -0.0269],\n",
       "        [ 0.7026, -1.0440, -1.3213,  0.3033,  0.1506],\n",
       "        [ 0.1498, -0.0740, -0.3236,  0.2132, -1.6511],\n",
       "        [-0.6272, -0.4069,  1.7836, -0.5955,  2.1632],\n",
       "        [ 0.6795, -0.4418,  1.4164,  1.1529, -0.5701],\n",
       "        [-2.1803, -1.4923,  0.0716, -1.6482,  1.2458]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs-inputs.mean(dim=0))/torch.sqrt(inputs.var(dim=0,unbiased=False)+batchNorm1.eps)*batchNorm1.weight + batchNorm1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6498b2",
   "metadata": {},
   "source": [
    "## BatchNorm2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db36526",
   "metadata": {},
   "source": [
    "BatchNorm2d applies batch Normalization over a 4D input with the shape of [Batch_size, in_channels,height,width]. The mean and standard-deviation are calculated per-dimension over the channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4813dace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0825, -0.2377],\n",
       "          [ 2.4320, -0.7050]],\n",
       "\n",
       "         [[-0.8265, -0.7808],\n",
       "          [ 1.2174, -1.3867]],\n",
       "\n",
       "         [[ 0.9693, -0.7551],\n",
       "          [-0.3473, -0.7084]]],\n",
       "\n",
       "\n",
       "        [[[-0.3053, -0.4256],\n",
       "          [-1.0903,  0.2493]],\n",
       "\n",
       "         [[-0.1257,  0.1201],\n",
       "          [ 1.8087, -0.0265]],\n",
       "\n",
       "         [[-1.2152, -0.6561],\n",
       "          [ 0.9815,  1.7313]]]], grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchNorm2d = nn.BatchNorm2d(3, affine=True) # the first parameter 3 is the in_channels of the input\n",
    "\n",
    "inputs = torch.randn(2,3,2,2) # in the shape of [batch_size, in_channles, height, width]\n",
    "outputs = batchNorm2d(inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e29745",
   "metadata": {},
   "source": [
    "Let's reproduct that with this fomular below\n",
    "$$ y = \\frac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}}*weight + bias$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ddc5dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the means and vars in each channels\n",
    "\n",
    "means = torch.tensor([inputs[:,0].mean(),inputs[:,1].mean(),inputs[:,2].mean()\n",
    "                     ]).view(-1,1,1).expand_as(inputs)\n",
    "vars = torch.tensor([inputs[:,0].var(unbiased=False),inputs[:,1].var(unbiased=False),inputs[:,2].var(unbiased=False)\n",
    "                     ]).view(-1,1,1).expand_as(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a935b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0825, -0.2377],\n",
       "          [ 2.4320, -0.7050]],\n",
       "\n",
       "         [[-0.8265, -0.7808],\n",
       "          [ 1.2174, -1.3867]],\n",
       "\n",
       "         [[ 0.9693, -0.7551],\n",
       "          [-0.3473, -0.7084]]],\n",
       "\n",
       "\n",
       "        [[[-0.3053, -0.4256],\n",
       "          [-1.0903,  0.2493]],\n",
       "\n",
       "         [[-0.1257,  0.1201],\n",
       "          [ 1.8087, -0.0265]],\n",
       "\n",
       "         [[-1.2152, -0.6561],\n",
       "          [ 0.9815,  1.7313]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchNorm2d.weight.view(-1,1,1).expand_as(inputs)*(inputs-means)/torch.sqrt(vars+batchNorm2d.eps)+ batchNorm2d.bias.view(-1,1,1).expand_as(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa1917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252146f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6334acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91461b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa334885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ecce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db240e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
